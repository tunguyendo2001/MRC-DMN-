{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23994,"status":"ok","timestamp":1664391285257,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"iJ4nu-LckW9l","outputId":"536b482a-7c45-407a-a8e6-be2b5a5c9835"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"66NM7GKIM6ta"},"source":["### LOADING PREPROCESSED DATA\n","\n","Loading GloVe word embeddings. Building functions to convert words into their vector representations and vice versa. Loading babi induction task 10K dataset."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1664391285261,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"Jxuj4mgVplwz"},"outputs":[],"source":["from __future__ import division"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38774,"status":"ok","timestamp":1664391324020,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"TV2fOFSiM6te","outputId":"4077e4f1-c568-4263-8e62-f363adb18f5f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded!\n"]}],"source":["import numpy as np\n","\n","filename = '/content/drive/MyDrive/Colab Notebooks/Project 3: Question Answering/Dynamic-Memory-Network-Plus/glove.6B.100d.txt'\n","\n","def loadEmbeddings(filename):\n","    vocab = []\n","    embd = []\n","    file = open(filename,'r')\n","    for line in file.readlines():\n","        row = line.strip().split(' ')\n","        vocab.append(row[0])\n","        embd.append(row[1:])\n","    print('Loaded!')\n","    file.close()\n","    return vocab,embd\n","vocab,embd = loadEmbeddings(filename)\n","\n","\n","word_vec_dim = len(embd[0])\n","\n","vocab.append('<UNK>')\n","embd.append(np.asarray(embd[vocab.index('unk')],np.float32)+0.01)\n","\n","vocab.append('<EOS>')\n","embd.append(np.asarray(embd[vocab.index('eos')],np.float32)+0.01)\n","\n","vocab.append('<PAD>')\n","embd.append(np.zeros((word_vec_dim),np.float32))\n","\n","embedding = np.asarray(embd)\n","embedding = embedding.astype(np.float32)\n","\n","def word2vec(word):  # converts a given word into its vector representation\n","    if word in vocab:\n","        return embedding[vocab.index(word)]\n","    else:\n","        return embedding[vocab.index('<UNK>')]\n","\n","def most_similar_eucli(x):\n","    xminusy = np.subtract(embedding,x)\n","    sq_xminusy = np.square(xminusy)\n","    sum_sq_xminusy = np.sum(sq_xminusy,1)\n","    eucli_dists = np.sqrt(sum_sq_xminusy)\n","    return np.argsort(eucli_dists)\n","\n","def vec2word(vec):   # converts a given vector representation into the represented word \n","    most_similars = most_similar_eucli(np.asarray(vec,np.float32))\n","    return vocab[most_similars[0]]"]},{"cell_type":"markdown","source":["### LOAD EMBEDDED DATASET"],"metadata":{"id":"KI7PRPkPICi6"}},{"cell_type":"code","source":["import pickle\n","\n","with open ('/content/drive/MyDrive/Colab Notebooks/Project 3: Question Answering/Dynamic-Memory-Network-Plus/embeddingPICKLE-3', 'rb') as fp:\n","    processed_data = pickle.load(fp)\n","\n","fact_stories = processed_data[0]\n","questions = processed_data[1]\n","answers = np.reshape(processed_data[2],len(processed_data[2]))\n","test_fact_stories = processed_data[3]\n","test_questions = processed_data[4]\n","test_answers = np.reshape(processed_data[5],len(processed_data[5]))"],"metadata":{"id":"ELOYvuOIIA6H","executionInfo":{"status":"ok","timestamp":1664391358051,"user_tz":-420,"elapsed":34048,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1664391358052,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"B7JPPh_WiG3b","outputId":"411c92de-1b04-4cd6-dcaa-668b301f381b"},"outputs":[{"output_type":"stream","name":"stdout","text":["4906.0\n","26960\n","26\n"]}],"source":["print(answers[1])\n","print(len(fact_stories))\n","print(len(test_fact_stories[2]))"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27355,"status":"ok","timestamp":1664391385389,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"oUlNX0YiM6tf","outputId":"31710081-33cf-4593-b8bc-1767964d42c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["EXAMPLE DATA:\n","\n","10\n","FACTS:\n","\n","1) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","2) \n","['mary', 'picked', 'up', 'the', 'apple', '<PAD>']\n","3) \n","['john', 'went', 'to', 'the', 'office', '<PAD>']\n","4) \n","['mary', 'journeyed', 'to', 'the', 'garden', '<PAD>']\n","5) \n","['mary', 'went', 'to', 'the', 'bedroom', '<PAD>']\n","6) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","7) \n","['sandra', 'moved', 'to', 'the', 'garden', '<PAD>']\n","8) \n","['mary', 'dropped', 'the', 'apple', '<PAD>', '<PAD>']\n","9) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","10) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","11) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","12) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","13) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","14) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","15) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","16) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","17) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","18) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","19) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","20) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","21) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","22) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","23) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","24) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","25) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","26) \n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","\n","QUESTION:\n","['where', 'was', 'the', 'apple', 'before', 'the', 'bedroom']\n","\n","ANSWER:\n","garden\n"]}],"source":["import random\n","\n","print(\"EXAMPLE DATA:\\n\")\n","\n","# sample = random.randint(0,len(fact_stories))\n","sample = 10\n","print(sample)\n","print(\"FACTS:\\n\")\n","for i in range(len(fact_stories[sample])):\n","    print(str(i+1)+\") \",)\n","    print(list(map(vec2word,fact_stories[sample][i])))\n","    \n","print(\"\\nQUESTION:\")\n","print(list(map(vec2word,questions[sample])))\n","print(\"\\nANSWER:\")\n","print(vocab[int(answers[sample])])"]},{"cell_type":"markdown","metadata":{"id":"LLTxQx7CM6tg"},"source":["### CREATING TRAINING AND VALIDATION DATA"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1664391385390,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"Mz55sDSmM6tg"},"outputs":[],"source":["from __future__ import division\n","\n","train_fact_stories = []\n","train_questions = []\n","train_answers = []\n","val_fact_stories = []\n","val_questions = []\n","val_answers = []\n","\n","p=90 #(90% data used for training. Rest for validation)\n","    \n","train_len = int((p/100)*len(fact_stories))\n","\n","train_fact_stories = fact_stories[0:train_len] \n","val_fact_stories = fact_stories[train_len:]\n","\n","train_questions = questions[0:train_len] \n","val_questions = questions[train_len:] \n","\n","train_answers = answers[0:train_len] \n","val_answers = answers[train_len:] \n","\n"]},{"cell_type":"markdown","metadata":{"id":"2SlyCyg3M6tg"},"source":["### SENTENCE READING LAYER IMPLEMENTED BEFOREHAND \n","\n","Positionally encode the word vectors in each sentence, and combine all the words in the sentence to create a fixed sized vector representation for the sentence.\n","\n","\"sentence embedding\""]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":356118,"status":"ok","timestamp":1664391741493,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"DVd4jHetM6th"},"outputs":[],"source":["# from __future__ import division\n","\n","def sentence_reader(fact_stories): #positional_encoder\n","    \n","    PAD_val = np.zeros((word_vec_dim),np.float32)\n","    \n","    pe_fact_stories = np.zeros((fact_stories.shape[0],fact_stories.shape[1],word_vec_dim),np.float32)\n","    \n","    for fact_story_index in range(0,len(fact_stories)):\n","        for fact_index in range(0,len(fact_stories[fact_story_index])):\n","            \n","            M = 0\n","            \n","            # Code to ignore pads. \n","            for word_position in range(len(fact_stories[fact_story_index,fact_index])):\n","                if np.all(np.equal(PAD_val,fact_stories[fact_story_index,fact_index,word_position])):\n","                    break\n","                else:\n","                    M+=1\n","                \n","            l = np.zeros((word_vec_dim),np.float32) \n","            \n","            # ljd = (1 − j/M) − (d/D)(1 − 2j/M),\n","            \n","            for word_position in range(0,M):\n","                \n","                for dimension in range(0,word_vec_dim):\n","                    \n","                    j = word_position + 1 # making position start from 1 instead of 0\n","                    d = dimension + 1 # making dimensions start from 1 isntead of 0 (1-100 instead of 0-99)\n","                    \n","                    l[dimension] = (1-(j/M)) - (d/word_vec_dim)*(1-2*(j/M))\n"," \n","                \n","                pe_fact_stories[fact_story_index,fact_index] += np.multiply(l,fact_stories[fact_story_index,fact_index,word_position])\n","\n","\n","    return pe_fact_stories\n","\n","train_fact_stories = sentence_reader(train_fact_stories)\n","val_fact_stories = sentence_reader(val_fact_stories)\n","test_fact_stories = sentence_reader(test_fact_stories)\n","                \n","        "]},{"cell_type":"markdown","metadata":{"id":"fa03gk8nM6th"},"source":["### Function to create randomized batches"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1664391741494,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"zc0hHZoqM6ti"},"outputs":[],"source":["def create_batches(fact_stories,questions,answers,batch_size):\n","    \n","    shuffle = np.arange(len(questions))\n","    np.random.shuffle(shuffle)\n","    \n","    batches_fact_stories = []\n","    batches_questions = []\n","    batches_answers = []\n","    \n","    i=0\n","    \n","    while i+batch_size<=len(questions):\n","        batch_fact_stories = []\n","        batch_questions = []\n","        batch_answers = []\n","        \n","        for j in range(i,i+batch_size):\n","            batch_fact_stories.append(fact_stories[shuffle[j]])\n","            batch_questions.append(questions[shuffle[j]])\n","            batch_answers.append(answers[shuffle[j]])\n","            \n","        batch_fact_stories = np.asarray(batch_fact_stories,np.float32)\n","        batch_fact_stories = np.transpose(batch_fact_stories,[1,0,2])\n","        #result = number of facts x batch_size x fact sentence size x word vector size\n","        \n","        batch_questions = np.asarray(batch_questions,np.float32)\n","        batch_questions = np.transpose(batch_questions,[1,0,2])\n","        #result = question_length x batch_size x fact sentence size x word vector size\n","        \n","        batches_fact_stories.append(batch_fact_stories)\n","        batches_questions.append(batch_questions)\n","        batches_answers.append(batch_answers)\n","        \n","        i+=batch_size\n","        \n","    batches_fact_stories = np.asarray(batches_fact_stories,np.float32)\n","    batches_questions = np.asarray(batches_questions,np.float32)\n","    batches_answers = np.asarray(batches_answers,np.int32)\n","    \n","    return batches_fact_stories,batches_questions,batches_answers\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"rZXTYxBiM6ti"},"source":["### Hyperparameters"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15944,"status":"ok","timestamp":1664391757415,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"QpgvX0hEM6ti","outputId":"3f745d45-d854-46f9-fa3b-06d4bfc78ef0"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]}],"source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","\n","# Tensorflow placeholders\n","\n","tf_facts = tf.placeholder(tf.float32,[None,None,word_vec_dim])\n","tf_questions = tf.placeholder(tf.float32,[None,None,word_vec_dim])\n","tf_answers = tf.placeholder(tf.int32,[None])\n","training = tf.placeholder(tf.bool)\n","\n","#hyperparameters\n","epochs = 256\n","learning_rate = 0.001\n","hidden_size = 100\n","passes = 3\n","dropout_rate = 0.1\n","beta = 0.0001 #l2 regularization scale\n","\n","regularizer = tf.keras.regularizers.L2(l2=beta) #l2\n"]},{"cell_type":"markdown","metadata":{"id":"GztyX9s2M6tj"},"source":["### All the trainable parameters initialized here"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1664391757416,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"vV6NhesD8o3K"},"outputs":[],"source":["# tf.reset_default_graph()"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":630,"status":"ok","timestamp":1664391758025,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"hL4CGDyqM6tj"},"outputs":[],"source":["# Parameters\n","\n","# FORWARD GRU PARAMETERS FOR INPUT MODULE\n","\n","wf = tf.get_variable(\"wf\", shape=[3,word_vec_dim, hidden_size],\n","                      initializer=tf.keras.initializers.glorot_normal(),\n","                      regularizer= regularizer)\n","uf = tf.get_variable(\"uf\", shape=[3,hidden_size, hidden_size],\n","                      initializer=tf.keras.initializers.glorot_normal(),\n","                      regularizer=regularizer)\n","bf = tf.get_variable(\"bf\", shape=[3,hidden_size],initializer=tf.zeros_initializer())\n","\n","\n","# BACKWARD GRU PARAMETERS FOR INPUT MODULE\n","\n","wb = tf.get_variable(\"wb\", shape=[3,word_vec_dim, hidden_size],\n","                      initializer=tf.keras.initializers.glorot_normal(),\n","                      regularizer=regularizer)\n","ub = tf.get_variable(\"ub\", shape=[3,hidden_size, hidden_size],\n","                      initializer=tf.keras.initializers.glorot_normal(),\n","                      regularizer=regularizer)\n","bb = tf.get_variable(\"bb\", shape=[3,hidden_size],initializer=tf.zeros_initializer())\n","\n","# GRU PARAMETERS FOR QUESTION MODULE (TO ENCODE THE QUESTIONS)\n","\n","wq = tf.get_variable(\"wq\", shape=[3,word_vec_dim, hidden_size],\n","                      initializer=tf.keras.initializers.glorot_normal(),\n","                      regularizer=regularizer)\n","uq = tf.get_variable(\"uq\", shape=[3,hidden_size, hidden_size],\n","                      initializer=tf.keras.initializers.glorot_normal(),\n","                      regularizer=regularizer)\n","bq = tf.get_variable(\"bq\", shape=[3,hidden_size],initializer=tf.zeros_initializer())\n","\n","\n","# EPISODIC MEMORY\n","\n","# ATTENTION MECHANISM\n","\n","inter_neurons = hidden_size\n","\n","w1 = tf.get_variable(\"w1\", shape=[hidden_size*4, inter_neurons],\n","                     initializer=tf.keras.initializers.glorot_normal(),\n","                     regularizer=regularizer)\n","b1 = tf.get_variable(\"b1\", shape=[inter_neurons],\n","                     initializer=tf.zeros_initializer())\n","w2 = tf.get_variable(\"w2\", shape=[inter_neurons,1],\n","                     initializer=tf.keras.initializers.glorot_normal(),\n","                     regularizer=regularizer)\n","b2 = tf.get_variable(\"b2\", shape=[1],initializer=tf.zeros_initializer())\n","\n","\n","# ATTENTION BASED GRU PARAMETERS\n","\n","watt = tf.get_variable(\"watt\", shape=[2,hidden_size,hidden_size],\n","                       initializer=tf.keras.initializers.glorot_normal(),\n","                       regularizer=regularizer)\n","uatt = tf.get_variable(\"uatt\", shape=[2,hidden_size, hidden_size],\n","                      initializer=tf.keras.initializers.glorot_normal(),\n","                       regularizer=regularizer)\n","batt = tf.get_variable(\"batt\", shape=[2,hidden_size],initializer=tf.zeros_initializer())\n","\n","\n","# MEMORY UPDATE PARAMETERS\n","# (UNTIED)\n","\n","wt = tf.get_variable(\"wt\", shape=[passes,hidden_size*3,hidden_size],\n","                    initializer=tf.keras.initializers.glorot_normal(),\n","                    regularizer=regularizer)\n","bt = tf.get_variable(\"bt\", shape=[passes,hidden_size],\n","                     initializer=tf.zeros_initializer())\n","\n","# ANSWER MODULE PARAMETERS\n","\n","wa_pd = tf.get_variable(\"wa_pd\", shape=[hidden_size*2,len(vocab)],\n","                     initializer=tf.keras.initializers.glorot_normal(),\n","                     regularizer=regularizer)\n","ba_pd = tf.get_variable(\"ba_pd\", shape=[len(vocab)],\n","                     initializer=tf.zeros_initializer())\n"]},{"cell_type":"markdown","metadata":{"id":"m8Mf6gFMM6tj"},"source":["### Layer Normalization"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1664391758026,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"9zAZuHCTM6tj"},"outputs":[],"source":["\n","def layer_norm(inputs,scope,scale=True,layer_norm=True,epsilon = 1e-5):\n","    \n","    if layer_norm == True:\n","        \n","        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n","            \n","            if scale == False:\n","                scale = tf.ones([inputs.get_shape()[1]],tf.float32)\n","            else:\n","                scale = tf.get_variable(\"scale\", shape=[inputs.get_shape()[1]],\n","                        initializer=tf.ones_initializer())\n","        \n","        \n","        ## ignored shift - bias will be externally added which can produce shift\n","\n","        mean, var = tf.nn.moments(inputs, [1], keep_dims=True)\n","        \n","        LN = tf.multiply((scale / tf.sqrt(var + epsilon)),(inputs - mean))\n","        \n","        return LN\n","    else:\n","        return inputs"]},{"cell_type":"markdown","metadata":{"id":"gqgZKo9yM6tk"},"source":["###  GRU Function\n","\n","Returns a tensor of all the hidden states"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1664391758026,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"R85acn8jM6tk"},"outputs":[],"source":["def GRU(inp,hidden,\n","        w,u,b,\n","        seq_len,scope):\n","\n","    hidden_lists = tf.TensorArray(size=seq_len,dtype=tf.float32)\n","    \n","    i=0\n","    \n","    def cond(i,hidden,hidden_lists):\n","        return i < seq_len\n","    \n","    def body(i,hidden,hidden_lists):\n","        \n","        x = inp[i]\n"," \n","        # GRU EQUATIONS:\n","        z = tf.sigmoid(layer_norm( tf.matmul(x,w[0]) + tf.matmul(hidden,u[0]) , scope+\"_z\")+ b[0])\n","        r = tf.sigmoid(layer_norm( tf.matmul(x,w[1]) + tf.matmul(hidden,u[1]) , scope+\"_r\")+ b[1])\n","        h_ = tf.tanh(layer_norm( tf.matmul(x,w[2]) + tf.multiply(r,tf.matmul(hidden,u[2])),scope+\"_h\") + b[2])\n","        hidden = tf.multiply(z,h_) + tf.multiply((1-z),hidden)\n","\n","        hidden_lists = hidden_lists.write(i,hidden)\n","        \n","        return i+1,hidden,hidden_lists\n","    \n","    _,_,hidden_lists = tf.while_loop(cond,body,[i,hidden,hidden_lists])\n","    \n","    return hidden_lists.stack()\n","        "]},{"cell_type":"markdown","metadata":{"id":"WpL7xUDLM6tk"},"source":["### Attention based GRU\n","\n","Returns only the final hidden state."]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1664391758026,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"m-NgDp9AM6tk"},"outputs":[],"source":["def attention_based_GRU(inp,hidden,\n","                        w,u,b,\n","                        g,seq_len,scope):\n","    \n","    i=0\n","    \n","    def cond(i,hidden):\n","        return i < seq_len\n","    \n","    def body(i,hidden):\n","        \n","        x = inp[i]\n","\n","        # GRU EQUATIONS:\n","        r = tf.sigmoid(layer_norm( tf.matmul(x,w[0]) + tf.matmul(hidden,u[0]), scope+\"_r\") + b[0])\n","        h_ = tf.tanh(layer_norm( tf.matmul(x,w[1]) + tf.multiply(r,tf.matmul(hidden,u[1])),scope+\"_h\") + b[1])\n","        hidden = tf.multiply(g[i],h_) + tf.multiply((1-g[i]),hidden)\n","        \n","        return i+1,hidden\n","    \n","    _,hidden = tf.while_loop(cond,body,[i,hidden])\n","    \n","    return hidden"]},{"cell_type":"markdown","metadata":{"id":"bvIoGU7SM6tk"},"source":["### Dynamic Memory Network+ Model Definition"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1664391758028,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"C8ctV2FgM6tl"},"outputs":[],"source":["def DMN_plus(tf_facts,tf_questions):\n","    \n","    facts_num = tf.shape(tf_facts)[0]\n","    tf_batch_size = tf.shape(tf_questions)[1]\n","    question_len = tf.shape(tf_questions)[0]\n","    \n","    hidden = tf.zeros([tf_batch_size,hidden_size],tf.float32)\n","\n","    # Input Module\n","\n","    # input fusion layer \n","    # bidirectional GRU\n","    \n","    forward = GRU(tf_facts,hidden,\n","                  wf,uf,bf,\n","                  facts_num,\"Forward_GRU\")\n","    \n","    backward = GRU(tf.reverse(tf_facts,[0]),hidden,\n","                   wb,ub,bb,\n","                   facts_num,\"Backward_GRU\")\n","    \n","    backward = tf.reverse(backward,[0])\n","    \n","    encoded_input = tf.add(forward,backward)\n","    \n","    # encoded input now shape = facts_num x batch_size x hidden_size\n","    \n","    encoded_input = tf.layers.dropout(encoded_input,dropout_rate,training=training)\n","\n","    # Question Module\n","    \n","    question_representation = GRU(tf_questions,hidden,\n","                                  wq,uq,bq,\n","                                  question_len,\"Question_GRU\")\n","    \n","    #question_representation's current shape = question len x batch size x hidden size\n","    \n","    question_representation = question_representation[question_len-1]\n","    \n","    #^we will only use the final hidden state. \n","\n","    question_representation = tf.reshape(question_representation,[tf_batch_size,1,hidden_size])\n","    \n","    # Episodic Memory Module\n","    \n","    episodic_memory = tf.identity(question_representation)\n","    \n","    encoded_input = tf.transpose(encoded_input,[1,0,2])\n","    #now shape = batch_size x facts_num x hidden_size\n","    \n","\n","    for i in range(passes):\n","        \n","        # Attention Mechanism\n","        \n","        Z1 = tf.multiply(encoded_input,question_representation)\n","        Z2 = tf.multiply(encoded_input,episodic_memory)\n","        Z3 = tf.abs(tf.subtract(encoded_input,question_representation))\n","        Z4 = tf.abs(tf.subtract(encoded_input,episodic_memory))\n","        \n","        Z = tf.concat([Z1,Z2,Z3,Z4],2)\n","        \n","        Z = tf.reshape(Z,[-1,4*hidden_size])\n","        Z = tf.matmul( tf.tanh( layer_norm( tf.matmul(Z,w1), \"Attention_Mechanism\") + b1),w2 ) \n","        Z = layer_norm(Z,\"Attention_Mechanism_2\") + b2\n","        Z = tf.reshape(Z,[tf_batch_size,1,facts_num])\n","        \n","        g = tf.nn.softmax(Z)\n","\n","        g = tf.transpose(g,[2,0,1])\n","        \n","        context_vector = attention_based_GRU(tf.transpose(encoded_input,[1,0,2]),\n","                                             hidden,\n","                                             watt,uatt,batt,\n","                                             g,facts_num,\"Attention_GRU\")\n","                                             \n","        \n","        context_vector = tf.reshape(context_vector,[tf_batch_size,1,hidden_size])\n","        \n","        # Episodic Memory Update\n","        \n","        concated = tf.concat([episodic_memory,context_vector,question_representation],2)\n","        concated = tf.reshape(concated,[-1,3*hidden_size])\n","        \n","        episodic_memory = tf.nn.relu(layer_norm(tf.matmul(concated,wt[i]),\"Memory_Update\",scale=False) + bt[i])\n","        \n","        episodic_memory = tf.reshape(episodic_memory,[tf_batch_size,1,hidden_size])\n","\n","    # Answer module \n","    \n","    # (single word answer prediction)\n","\n","    episodic_memory = tf.reshape(episodic_memory,[tf_batch_size,hidden_size])\n","    episodic_memory = tf.layers.dropout(episodic_memory,dropout_rate,training=training)\n","\n","    question_representation = tf.reshape(question_representation,[tf_batch_size,hidden_size])\n","    \n","    y_concat = tf.concat([question_representation,episodic_memory],1)\n","    \n","    # Convert to pre-softmax probability distribution\n","    y = tf.matmul(y_concat,wa_pd) + ba_pd\n","    return y"]},{"cell_type":"markdown","metadata":{"id":"PVm369ouM6tl"},"source":["### Cost function, Evaluation, Optimization function "]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1664391758029,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"62Xamf4hz-1D","outputId":"4888bc7b-ef75-4d05-bc99-43bcf20bf7d5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['wf:0',\n"," 'uf:0',\n"," 'bf:0',\n"," 'wb:0',\n"," 'ub:0',\n"," 'bb:0',\n"," 'wq:0',\n"," 'uq:0',\n"," 'bq:0',\n"," 'w1:0',\n"," 'b1:0',\n"," 'w2:0',\n"," 'b2:0',\n"," 'watt:0',\n"," 'uatt:0',\n"," 'batt:0',\n"," 'wt:0',\n"," 'bt:0',\n"," 'wa_pd:0',\n"," 'ba_pd:0']"]},"metadata":{},"execution_count":17}],"source":["[i.name for i in tf.trainable_variables()]"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7380,"status":"ok","timestamp":1664391765399,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"},"user_tz":-420},"id":"Y0DDuwohM6tl","outputId":"d4ddeeb3-390f-44a5-fb69-00d2dac535c7"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n","/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:413: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  return layer.apply(inputs, training=training)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:94: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n"]}],"source":["model_output = DMN_plus(tf_facts,tf_questions)\n","\n","\n","# l2 regularization\n","reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n","# regularization = tf.layers.apply_regularization(regularizer, tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n","# l2_norms = [tf.nn.l2_loss(v) for v in [wf, uf, wb, ub, wq, uq, w1, w2, watt, uatt, wt, wa_pd]]\n","l2_norms = [tf.nn.l2_loss(v) for v in tf.trainable_variables()]\n","l2_norm = tf.reduce_sum(l2_norms)\n","regularization = beta*l2_norm\n","\n","# Define loss and optimizer\n","cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, \n","                                                                     labels=tf_answers))+regularization\n","# cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, \n","#                                                                      labels=tf_answers))\n","\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","model_output = tf.nn.softmax(model_output)\n","\n","#Evaluate model\n","correct_pred = tf.equal(tf.cast(tf.argmax(model_output,1),tf.int32),tf_answers)\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","\n","# Initializing the variables\n","init = tf.global_variables_initializer()"]},{"cell_type":"markdown","metadata":{"id":"JLfGQDixM6tl"},"source":["### Training...."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"MTpRsu0vM6tl","scrolled":true,"executionInfo":{"status":"error","timestamp":1664392276275,"user_tz":-420,"elapsed":510894,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"}},"outputId":"8ae559c7-d9e0-4571-8bfd-a4a58601b88e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Iter 0, Loss= 13.038, Accuracy= 0.000\n","Iter 20, Loss= nan, Accuracy= 0.000\n","Iter 40, Loss= nan, Accuracy= 0.000\n","Iter 60, Loss= nan, Accuracy= 0.000\n","Iter 80, Loss= nan, Accuracy= 0.000\n","Iter 100, Loss= nan, Accuracy= 0.000\n","Iter 120, Loss= nan, Accuracy= 0.000\n","Iter 140, Loss= nan, Accuracy= 0.000\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-19-b951da55bba1>\", line 37, in <module>\n","    training: True})\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 968, in run\n","    run_metadata_ptr)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1191, in _run\n","    feed_dict_tensor, options, run_metadata)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1371, in _do_run\n","    run_metadata)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1377, in _do_call\n","    return fn(*args)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1361, in _run_fn\n","    target_list, run_metadata)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1455, in _call_tf_sessionrun\n","    run_metadata)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n","    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n","  File \"/usr/lib/python3.7/inspect.py\", line 742, in getmodule\n","    os.path.realpath(f)] = module.__name__\n","  File \"/usr/lib/python3.7/posixpath.py\", line 395, in realpath\n","    path, ok = _joinrealpath(filename[:0], filename, {})\n","  File \"/usr/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n","    if not islink(newpath):\n","  File \"/usr/lib/python3.7/posixpath.py\", line 171, in islink\n","    st = os.lstat(path)\n","KeyboardInterrupt\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}],"source":["with tf.Session() as sess: # Start Tensorflow Session\n","    \n","    saver = tf.train.Saver() \n","\n","    sess.run(init) #initialize all variables\n","    step = 1   \n","    loss_list=[]\n","    acc_list=[]\n","    val_loss_list=[]\n","    val_acc_list=[]\n","    best_val_acc=0\n","    best_val_loss=2**30\n","    prev_val_acc=0\n","    patience = 20\n","    impatience = 0\n","    display_step = 20\n","    min_epoch = 20\n","            \n","    batch_size = 128\n","    \n","    while step <= epochs:\n","        \n","        total_loss=0\n","        total_acc=0\n","        total_val_loss = 0\n","        total_val_acc = 0\n","\n","        batches_train_fact_stories,batches_train_questions,batches_train_answers = create_batches(train_fact_stories,train_questions,train_answers,batch_size)\n","        \n","        for i in range(len(batches_train_questions)):\n","            \n","            # Run optimization operation (backpropagation)\n","            _,loss,acc = sess.run([optimizer,cost,accuracy],\n","                                       feed_dict={tf_facts: batches_train_fact_stories[i], \n","                                                  tf_questions: batches_train_questions[i], \n","                                                  tf_answers: batches_train_answers[i],\n","                                                  training: True})\n","\n","            total_loss += loss\n","            total_acc += acc\n","                \n","            if i%display_step == 0:\n","                print(\"Iter \"+str(i)+\", Loss= \"+\\\n","                      \"{:.3f}\".format(loss)+\", Accuracy= \"+\\\n","                      \"{:.3f}\".format(acc*100))\n","                        \n","        avg_loss = total_loss/len(batches_train_questions) \n","        avg_acc = total_acc/len(batches_train_questions)  \n","        \n","        loss_list.append(avg_loss) \n","        acc_list.append(avg_acc) \n","\n","        val_batch_size = 100 #(should be able to divide total no. of validation samples without remainder)\n","        batches_val_fact_stories,batches_val_questions,batches_val_answers = create_batches(val_fact_stories,val_questions,val_answers,val_batch_size)\n","        \n","        for i in range(len(batches_val_questions)):\n","            val_loss, val_acc = sess.run([cost, accuracy], \n","                                         feed_dict={tf_facts: batches_val_fact_stories[i], \n","                                                    tf_questions: batches_val_questions[i], \n","                                                    tf_answers: batches_val_answers[i],\n","                                                    training: False})\n","            total_val_loss += val_loss\n","            total_val_acc += val_acc\n","                      \n","            \n","        avg_val_loss = total_val_loss/len(batches_val_questions) \n","        avg_val_acc = total_val_acc/len(batches_val_questions) \n","             \n","        val_loss_list.append(avg_val_loss) \n","        val_acc_list.append(avg_val_acc) \n","    \n","\n","        print(\"\\nEpoch \" + str(step) + \", Validation Loss= \" + \\\n","                \"{:.3f}\".format(avg_val_loss) + \", validation Accuracy= \" + \\\n","                \"{:.3f}%\".format(avg_val_acc*100)+\"\")\n","        print(\"Epoch \" + str(step) + \", Average Training Loss= \" + \\\n","              \"{:.3f}\".format(avg_loss) + \", Average Training Accuracy= \" + \\\n","              \"{:.3f}%\".format(avg_acc*100)+\"\")\n","        \n","        impatience += 1\n","        \n","        if avg_val_acc >= best_val_acc:\n","            best_val_acc = avg_val_acc\n","            saver.save(sess, 'DMN_Model_Backup/model.ckpt') \n","            print(\"Checkpoint created!\")\n","\n","\n","    \n","        if avg_val_loss <= best_val_loss: \n","            impatience=0\n","            best_val_loss = avg_val_loss\n","\n","\n","        \n","        if impatience > patience and step>min_epoch:\n","            print(\"\\nEarly Stopping since best validation loss not decreasing for \"+str(patience)+\" epochs.\")\n","            break\n","            \n","        print(\"\")\n","        step += 1\n","        \n","    \n","        \n","    print(\"\\nOptimization Finished!\\n\")\n","    \n","    print(\"Best Validation Accuracy: %.3f%%\"%((best_val_acc*100)))\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGVPy0_AM6tm","executionInfo":{"status":"aborted","timestamp":1664392276276,"user_tz":-420,"elapsed":13,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"}}},"outputs":[],"source":["#Saving logs about change of training and validation loss and accuracy over epochs in another file.\n","\n","import h5py\n","\n","file = h5py.File('Training_logs_DMN_plus.h5','w')\n","file.create_dataset('val_acc', data=np.array(val_acc_list))\n","file.create_dataset('val_loss', data=np.array(val_loss_list))\n","file.create_dataset('acc', data=np.array(acc_list))\n","file.create_dataset('loss', data=np.array(loss_list))\n","\n","file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HqBoIyarM6tm","executionInfo":{"status":"aborted","timestamp":1664392276277,"user_tz":-420,"elapsed":14,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"}}},"outputs":[],"source":["import h5py\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","log = h5py.File('Training_logs_DMN_plus.h5','r+') # Loading logs about change of training and validation loss and accuracy over epochs\n","\n","y1 = log['val_acc'][...]\n","y2 = log['acc'][...]\n","\n","x = np.arange(1,len(y1)+1,1) # (1 = starting epoch, len(y1) = no. of epochs, 1 = step) \n","\n","plt.plot(x,y1,'b',label='Validation Accuracy') \n","plt.plot(x,y2,'r',label='Training Accuracy')\n","plt.legend(loc='lower right')\n","plt.xlabel('epoch')\n","plt.show()\n","\n","y1 = log['val_loss'][...]\n","y2 = log['loss'][...]\n","\n","plt.plot(x,y1,'b',label='Validation Loss')\n","plt.plot(x,y2,'r',label='Training Loss')\n","plt.legend(loc='upper right')\n","plt.xlabel('epoch')\n","plt.show()\n","\n","log.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cMUl85oyM6tm","scrolled":true,"executionInfo":{"status":"aborted","timestamp":1664392276278,"user_tz":-420,"elapsed":15,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"}}},"outputs":[],"source":["with tf.Session() as sess: # Begin session\n","    \n","    print('Loading pre-trained weights for the model...')\n","    saver = tf.train.Saver()\n","    saver.restore(sess, 'DMN_Model_Backup/model.ckpt')\n","    sess.run(tf.global_variables())\n","    print('\\nRESTORATION COMPLETE\\n')\n","    \n","    print('Testing Model Performance...')\n","    \n","    total_test_loss = 0\n","    total_test_acc = 0\n","    \n","    test_batch_size = 100 #(should be able to divide total no. of test samples without remainder)\n","    batches_test_fact_stories,batches_test_questions,batches_test_answers = create_batches(test_fact_stories,test_questions,test_answers,test_batch_size)\n","        \n","    for i in range(len(batches_test_questions)):\n","        test_loss, test_acc = sess.run([cost, accuracy], \n","                                        feed_dict={tf_facts: batches_test_fact_stories[i], \n","                                                   tf_questions: batches_test_questions[i], \n","                                                   tf_answers: batches_test_answers[i],\n","                                                   training: False})\n","        total_test_loss += test_loss\n","        total_test_acc += test_acc\n","                      \n","            \n","    avg_test_loss = total_test_loss/len(batches_test_questions) \n","    avg_test_acc = total_test_acc/len(batches_test_questions) \n","\n","\n","    print(\"\\nTest Loss= \" + \\\n","          \"{:.3f}\".format(avg_test_loss) + \", Test Accuracy= \" + \\\n","          \"{:.3f}%\".format(avg_test_acc*100)+\"\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BmTWO-PM6tm","executionInfo":{"status":"aborted","timestamp":1664392276278,"user_tz":-420,"elapsed":15,"user":{"displayName":"Nguyễn Đỗ Tú","userId":"01909318016181998346"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.12"}},"nbformat":4,"nbformat_minor":0}